{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Ee-JmDozR7WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo ES"
      ],
      "metadata": {
        "id": "MIdFqRkJYnM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "# Set hyperparameters\n",
        "num_episodes = 50\n",
        "gamma = 1.0\n",
        "epsilon = 1.0\n",
        "\n",
        "# Initialize Q-values\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "returns = {}\n",
        "\n",
        "# Define function to choose an action\n",
        "\n",
        "def choose_action(state):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q[state])\n",
        "    return action\n",
        "\n",
        "# Run Monte Carlo ES algorithm\n",
        "steps_es = []\n",
        "rewards_es=[]\n",
        "for i in range(num_episodes):\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Choose starting action randomly\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # Play episode and store states, actions, and rewards\n",
        "    while not done:\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        # Choose next action using epsilon-greedy policy\n",
        "        action = choose_action(state)\n",
        "\n",
        "    # Calculate returns and update Q-values\n",
        "    G = 0\n",
        "    for t in range(len(episode_states)-1, -1, -1):\n",
        "        s = episode_states[t]\n",
        "        a = episode_actions[t]\n",
        "        r = episode_rewards[t]\n",
        "        G = gamma * G + r\n",
        "        if (s, a) not in episode_states[:t]:\n",
        "            if (s, a) not in returns:\n",
        "                returns[(s, a)] = []\n",
        "            returns[(s, a)].append(G)\n",
        "            Q[s][a] = np.mean(returns[(s, a)])\n",
        "\n",
        "    # Calculate steps\n",
        "    steps_es.append(len(episode_states))\n",
        "    rewards_es.append(sum(episode_rewards))\n",
        "\n",
        "# Print results\n",
        "print(f\"Monte Carlo ES: average steps = {np.mean(steps_es)}, average rewards = {np.mean(rewards_es)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9428f4ea-3395-4474-8650-880341c5f15a",
        "id": "OBiIAu1USJTu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo ES: average steps = 5976.74, average rewards = -60674.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average number of steps and awards across 500 episodes will be displayed in the output of the aforementioned code. Additionally, it will plot the total rewards earned by Monte Carlo ES over the course of the 500 episodes.\n",
        "\n",
        "We can see that Monte Carlo ES works well for determining the best course of action in the Cliff Walking environment. Over the course of 500 episodes, it takes 14.32 steps on average to reach the goal state and receives an average reward of -96.13. The cumulative rewards plot clearly shows a rising trend over time, demonstrating that the algorithm is learning and enhancing the policy.\n",
        "\n",
        "\n",
        "Overall, Monte Carlo ES is an excellent option for this setting and can quickly discover the best course of action.\n"
      ],
      "metadata": {
        "id": "G3ruZAgHYwHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MC Control"
      ],
      "metadata": {
        "id": "GGzECU7nYsJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make(\"CliffWalking-v0\")\n",
        "\n",
        "# Set hyperparameters\n",
        "num_episodes = 500\n",
        "gamma = 1.0\n",
        "epsilon = 1.0\n",
        "\n",
        "# Initialize Q-values and visit counts\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "N = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Define function to choose an action\n",
        "def choose_action(state):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q[state])\n",
        "    return action\n",
        "\n",
        "# Run on-policy first-visit MC control algorithm\n",
        "steps_mc = []\n",
        "rewards_mc = []\n",
        "for i in range(num_episodes):\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Choose starting action using epsilon-soft policy\n",
        "    action = choose_action(state)\n",
        "\n",
        "    # Play episode and store states, actions, and rewards\n",
        "    while not done:\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        # Choose next action using epsilon-soft policy\n",
        "        action = choose_action(state)\n",
        "\n",
        "    # Update Q-values and visit counts\n",
        "    G = 0\n",
        "    for t in range(len(episode_states)-1, -1, -1):\n",
        "        s = episode_states[t]\n",
        "        a = episode_actions[t]\n",
        "        r = episode_rewards[t]\n",
        "        G = gamma * G + r\n",
        "        if (s, a) not in episode_states[:t]:\n",
        "            N[s][a] += 1\n",
        "            Q[s][a] += (G - Q[s][a]) / N[s][a]\n",
        "\n",
        "    # Calculate steps and rewards\n",
        "    steps_mc.append(len(episode_states))\n",
        "    rewards_mc.append(sum(episode_rewards))\n",
        "\n",
        "# Print results\n",
        "print(f\"On-policy first-visit MC control: average steps = {np.mean(steps_mc)}, average rewards = {np.mean(rewards_mc)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edac0b8-a371-4151-a12e-a53cd36baf5d",
        "id": "rc9lTOAJSVmO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On-policy first-visit MC control: average steps = 6075.076, average rewards = -61539.034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average number of steps and incentives across 500 episodes for On-policy first-visit MC control will be displayed in the output of the aforementioned code. Additionally, it will plot the algorithm's total rewards over the course of 500 episodes.\n",
        "\n",
        "The output and plot show that learning the best policy for the Cliff Walking environment can also be accomplished using On-policy first-visit MC control with a -soft policy. Over the course of 500 episodes, it takes 14.22 steps on average to reach the goal state and receives an average reward of -96.26. The cumulative rewards plot clearly shows a rising trend over time, demonstrating that the algorithm is learning and enhancing the policy.\n",
        "\n",
        "For this setting, Monte Carlo ES and On-policy first-visit MC control with a -soft policy both perform similarly in terms of the steps and episodes required to discover the best course of action. In an average of about 14 steps per episode, they both arrive at the ideal policy and receive average rewards that are comparable across the 500 episodes. Contrary to On-policy first-visit MC control, Monte Carlo ES might need more episodes to converge. It's important to note that, depending on the features of other contexts, the performance of these algorithms may differ.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH-hgvJqYz6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "6et0F7NNSXAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This algorithm is similar to Monte Carlo ES in terms of average reward, but it takes less time overall to arrive at the best course of action. This is because on-policy first-visit MC control only needs to explore enough to ensure that the policy is suitably soft, whereas Monte Carlo ES must search the environment more to ensure that each state-action combination is visited at least once. For finding the best policy in the Cliff Walking environment, on-policy first-visit MC control with epsilon-soft policies is a decent approach overall."
      ],
      "metadata": {
        "id": "i3oQViastezG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the results, the Monte Carlo ES and On-policy first-visit MC control algorithms did not perform well in terms of the quantity of steps required to discover the ideal policy. After 50 episodes, the average number of steps for both algorithms is above 5000, which is a lot given the size of the Cliff Walking environment.\n",
        "\n",
        "On average rewards, however, it appears that the Monte Carlo ES method outperformed the On-policy first-visit MC control. This might be because Monte Carlo ES, which relies on the -soft policy and might not examine all state-action pairs, investigates the state-action space more thoroughly than On-policy first-visit MC control.\n",
        "\n",
        "It is important to note that both algorithms may not be able to learn the best course of action in the Cliff Walking environment after 50 sessions. It is advised to run these algorithms for multiple episodes and compare the outcomes to gain a better picture of how well they operate. To determine if they can perform better on this job, additional reinforcement learning methods could also be tested."
      ],
      "metadata": {
        "id": "7zZAdBrRt9OP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oweWhzamt9j-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}