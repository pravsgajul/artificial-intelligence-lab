{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZjeW7krq-b3"
   },
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "47Z0PZJYnXMB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_iteration(policy, environment, discount_factor, theta, max_iterations):\n",
    "    # Extract the number of states and actions from the policy matrix\n",
    "    num_states, num_actions = policy.shape\n",
    "\n",
    "    # Initialize the value function to zeros for all states\n",
    "    value_function = np.zeros(num_states)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # Policy Evaluation: Evaluate the current policy by iteratively updating the value function until it converges\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(num_states):\n",
    "                v = value_function[s]\n",
    "                new_value = 0\n",
    "                for a in range(num_actions):\n",
    "                    prob = policy[s][a]\n",
    "                    next_states_rewards = []\n",
    "                    for prob_next, next_state, reward, done in environment.P[s][a]:\n",
    "                        next_states_rewards.append(prob_next * (reward + discount_factor * value_function[next_state]))\n",
    "                    new_value += prob * np.sum(next_states_rewards)\n",
    "                value_function[s] = new_value\n",
    "                delta = max(delta, np.abs(v - value_function[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        # Policy Improvement: Improve the policy by greedily selecting the best action in each state\n",
    "        policy_stable = True\n",
    "        for s in range(num_states):\n",
    "            old_action = np.argmax(policy[s])\n",
    "            action_values = np.zeros(num_actions)\n",
    "            for a in range(num_actions):\n",
    "                for prob_next, next_state, reward, done in environment.P[s][a]:\n",
    "                    action_values[a] += prob_next * (reward + discount_factor * value_function[next_state])\n",
    "            best_action = np.argmax(action_values)\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(num_actions)[best_action]\n",
    "        \n",
    "        # If the policy has not changed in this iteration, we have found the optimal policy\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return policy, value_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OE2IDPB-nhTn",
    "outputId": "c10e8975-0b1d-40ed-f8f0-c7c739d09dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Value Function:\n",
      "[0.54202581 0.49880303 0.4706955  0.4568515  0.55845085 0.\n",
      " 0.35834799 0.         0.59179866 0.64307976 0.6152075  0.\n",
      " 0.         0.7417204  0.86283741 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Create the FrozenLake-v1 environment\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "# Initialize the policy matrix to a random policy\n",
    "policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "\n",
    "# Set the discount factor, convergence threshold, and maximum number of iterations\n",
    "discount_factor = 0.99\n",
    "theta = 1e-8\n",
    "max_iterations = 1000\n",
    "\n",
    "# Run the policy iteration algorithm\n",
    "optimal_policy, optimal_value_function = policy_iteration(policy, env, discount_factor, theta, max_iterations)\n",
    "\n",
    "# Print the optimal policy and value function\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDUTdUXzqh1X"
   },
   "source": [
    "Create a Value Iteration function with the following parameters\n",
    "\n",
    "a. environment: Initialized OpenAI gym environment object<br>\n",
    "b.discount_factor: MDP discount factor<br>\n",
    "c.theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number<br>\n",
    "d.max_iterations: Maximum number of iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5AXOHjO1nqn-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def value_iteration(env, discount_factor, theta, max_iterations):\n",
    "    # Extract the number of states and actions from the environment object\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize the value function as a vector of zeros with length equal to the number of states\n",
    "    value_function = np.zeros(num_states)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "        for s in range(num_states):\n",
    "            v = value_function[s]\n",
    "            action_values = np.zeros(num_actions)\n",
    "            for a in range(num_actions):\n",
    "                for prob_next, next_state, reward, done in env.P[s][a]:\n",
    "                    action_values[a] += prob_next * (reward + discount_factor * value_function[next_state])\n",
    "            best_value = np.max(action_values)\n",
    "            value_function[s] = best_value\n",
    "            delta = max(delta, np.abs(v - best_value))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Derive the optimal policy from the optimal value function\n",
    "    policy = np.zeros((num_states, num_actions))\n",
    "    for s in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            for prob_next, next_state, reward, done in env.P[s][a]:\n",
    "                action_values[a] += prob_next * (reward + discount_factor * value_function[next_state])\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[s, best_action] = 1.0\n",
    "\n",
    "    return policy, value_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQ1KhfK0qmS_",
    "outputId": "2b185b35-f967-4e70-c9dc-6793307e5b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Value Function:\n",
      " [0.54202581 0.49880303 0.47069551 0.4568515  0.55845085 0.\n",
      " 0.35834799 0.         0.59179866 0.64307976 0.6152075  0.\n",
      " 0.         0.7417204  0.86283741 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the FrozenLake-v1 environment\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "# Set the discount factor, threshold, and maximum number of iterations for value iteration\n",
    "discount_factor = 0.99\n",
    "theta = 1e-8\n",
    "max_iterations = 10000\n",
    "\n",
    "# Run the value iteration algorithm to obtain the optimal policy and value function\n",
    "policy, value_function = value_iteration(env, discount_factor, theta, max_iterations)\n",
    "\n",
    "# Print the optimal policy and value function\n",
    "print(\"Optimal Policy:\\n\", policy)\n",
    "print(\"Optimal Value Function:\\n\", value_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHTO4nMBrkpu"
   },
   "source": [
    "Compare  the number of  wins, average  return  after  1000  episodes andcomment  on which method performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6gSaYkEqomN",
    "outputId": "2b8738d9-79ed-415b-917c-48c670a47a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Results:\n",
      "Number of wins: 725\n",
      "Average return: 0.725\n",
      "Value Iteration Results:\n",
      "Number of wins: 736\n",
      "Average return: 0.736\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def run_policy(policy, env, num_episodes):\n",
    "    num_wins = 0\n",
    "    total_return = 0\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_return = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=policy[state])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_return += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_return += episode_return\n",
    "        if episode_return > 0:\n",
    "            num_wins += 1\n",
    "\n",
    "    return num_wins, total_return / num_episodes\n",
    "\n",
    "\n",
    "# Initialize the FrozenLake-v1 environment\n",
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "# Set the discount factor, threshold, and maximum number of iterations for policy and value iteration\n",
    "discount_factor = 0.99\n",
    "theta = 1e-8\n",
    "max_iterations = 10000\n",
    "\n",
    "# Run policy iteration to obtain the optimal policy and value function\n",
    "policy_pi, _ = policy_iteration(np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n, env, discount_factor, theta, max_iterations)\n",
    "\n",
    "# Run value iteration to obtain the optimal policy and value function\n",
    "_, value_vi = value_iteration(env, discount_factor, theta, max_iterations)\n",
    "policy_vi = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "for s in range(env.observation_space.n):\n",
    "    q_values = np.zeros(env.action_space.n)\n",
    "    for a in range(env.action_space.n):\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q_values[a] += prob * (reward + discount_factor * value_vi[next_state])\n",
    "    best_a = np.argmax(q_values)\n",
    "    policy_vi[s, best_a] = 1.0\n",
    "\n",
    "# Run the policies for 1000 episodes and compare the results\n",
    "num_episodes = 1000\n",
    "num_wins_pi, avg_return_pi = run_policy(policy_pi, env, num_episodes)\n",
    "num_wins_vi, avg_return_vi = run_policy(policy_vi, env, num_episodes)\n",
    "\n",
    "# Print the results\n",
    "print(\"Policy Iteration Results:\")\n",
    "print(\"Number of wins:\", num_wins_pi)\n",
    "print(\"Average return:\", avg_return_pi)\n",
    "\n",
    "print(\"Value Iteration Results:\")\n",
    "print(\"Number of wins:\", num_wins_vi)\n",
    "print(\"Average return:\", avg_return_vi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5H3NYEIrnkf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
