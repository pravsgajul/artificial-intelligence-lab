{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1sFqZLog5tn"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI6LnXRipTvh"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Ads_Optimisation.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKwMtChj0a0R"
      },
      "source": [
        "## MAB Formulation\n",
        "To maximize the total number of clicks we receive, each time a user visits the webpage, the MAB agent must decide which ad to display to the user based on the available data of ad click history. \n",
        "\n",
        "The goal of the MAB agent is to balance exploration (trying out different ads to gather more data) and exploitation (using the best performing ads to maximize clicks) to achieve the maximum total number of clicks over a certain period of time.\n",
        "\n",
        "Therefore, the MAB agent must continually update its knowledge of which ad is most likely to be clicked by a user and make decisions based on that knowledge to optimize the number of clicks received."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZkR6ksZ5mHB"
      },
      "source": [
        "## Random Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqnuiNhhpnIW"
      },
      "outputs": [],
      "source": [
        "# Implementing Random Selection\n",
        "import random\n",
        "N = 10000\n",
        "d = 10\n",
        "ads_selected = []\n",
        "total_reward = 0\n",
        "for n in range(0, N):\n",
        "    ad = random.randrange(d)\n",
        "    ads_selected.append(ad)\n",
        "    reward = dataset.values[n, ad]\n",
        "    total_reward = total_reward + reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuwkHUQKpn4u",
        "outputId": "538a4cd5-9b74-4f15-9b53-dffa2084750c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4    0.115\n",
              "0    0.110\n",
              "7    0.108\n",
              "9    0.104\n",
              "3    0.101\n",
              "2    0.098\n",
              "6    0.097\n",
              "5    0.094\n",
              "8    0.090\n",
              "1    0.083\n",
              "dtype: float64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.Series(ads_selected).tail(1000).value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed1vLJru5ph9"
      },
      "source": [
        "## Epsilon-Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpJth1Wm1JpR",
        "outputId": "34926375-c73c-4c21-c3bb-8df52410cf72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards obtained with epsilon = 0.01: 337.0\n",
            "Total rewards obtained with epsilon = 0.3: 405.0\n"
          ]
        }
      ],
      "source": [
        "np.seterr(invalid='ignore')\n",
        "ads = np.loadtxt('Ads_Optimisation.csv', delimiter=',', skiprows=1)\n",
        "\n",
        "# define the number of arms (ads) and the total number of time steps\n",
        "num_arms = ads.shape[1]\n",
        "num_steps = 2000\n",
        "\n",
        "# initialize the number of times each arm was pulled and the total rewards obtained\n",
        "num_pulls = np.zeros(num_arms)\n",
        "total_rewards = np.zeros(num_arms)\n",
        "\n",
        "# define the epsilon-greedy action function\n",
        "def epsilon_greedy_action(epsilon):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        # explore: choose a random arm\n",
        "        return np.random.randint(num_arms)\n",
        "    else:\n",
        "        # exploit: choose the arm with the highest estimated reward\n",
        "        return np.argmax(total_rewards / num_pulls)\n",
        "\n",
        "# un the bandit algorithm for each value of epsilon\n",
        "for epsilon in [0.01, 0.3]:\n",
        "\n",
        "    # reset the number of times each arm was pulled and the total rewards obtained\n",
        "    num_pulls = np.zeros(num_arms)\n",
        "    total_rewards = np.zeros(num_arms)\n",
        "\n",
        "    # run the bandit algorithm for the specified number of time steps\n",
        "    for t in range(num_steps):\n",
        "        # choose which arm to pull using the epsilon-greedy action\n",
        "        arm = epsilon_greedy_action(epsilon)\n",
        "        \n",
        "        # update the number of times the chosen arm was pulled and the total rewards obtained\n",
        "        reward = ads[t][arm]\n",
        "        num_pulls[arm] += 1\n",
        "        total_rewards[arm] += reward\n",
        "\n",
        "    # compute the total rewards obtained\n",
        "    print(\"Total rewards obtained with epsilon = {}: {}\".format(epsilon, np.sum(total_rewards)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dXy-oWh3C5B"
      },
      "source": [
        "Thus, choosing a higher value of epsilon can lead to a higher total reward because the algorithm explores more frequently, which can result in a better understanding of the expected reward of each arm. However, this approach may not be optimal in the long run, as it can lead to suboptimal decisions when the algorithm becomes too focused on exploration and does not exploit the highest-reward arm enough.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w9LNoU75tOe"
      },
      "source": [
        "## UCB Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8Ckn2Pf385E",
        "outputId": "0d2322af-21e3-47cc-eec5-38014233e28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards obtained with UCB algorithm (c = 1.5): 319.0\n"
          ]
        }
      ],
      "source": [
        "# initialize the number of times each arm was pulled and the total rewards obtained\n",
        "num_pulls = np.zeros(num_arms)\n",
        "total_rewards = np.zeros(num_arms)\n",
        "\n",
        "# run the UCB algorithm with the specified value of c\n",
        "c = 1.5\n",
        "for t in range(num_steps):\n",
        "    # Choose which arm to pull using the UCB action\n",
        "    if t < num_arms:\n",
        "        arm = t  # Choose each arm once in the first num_arms steps\n",
        "    else:\n",
        "        ucb_values = total_rewards / num_pulls + c * np.sqrt(np.log(t) / num_pulls)\n",
        "        arm = np.argmax(ucb_values)\n",
        "        \n",
        "    # Update the number of times the chosen arm was pulled and the total rewards obtained\n",
        "    reward = ads[t][arm]\n",
        "    num_pulls[arm] += 1\n",
        "    total_rewards[arm] += reward\n",
        "\n",
        "# Compute the total rewards obtained\n",
        "print(\"Total rewards obtained with UCB algorithm (c = 1.5): {}\".format(np.sum(total_rewards)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPtSPf7v5hkx"
      },
      "source": [
        "In UCB, the algorithm tends to explore less often as it becomes more confident about the rewards of each arm. If the UCB algorithm becomes too confident too quickly, it may miss out on better rewards that could be obtained by exploring more.\n",
        "\n",
        "On the other hand, the epsilon-greedy algorithm always explores with a fixed probability, regardless of how confident it is about the rewards of each arm. This can lead to more exploration and potentially better rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdOO4HX16BNL"
      },
      "source": [
        "## Action Value vs Optimal Action\n",
        "\n",
        "- **Epsilon-greedy approach:** The action value estimated is the sample average of the rewards obtained for each action. The optimal action is the action that has the highest estimated action value. However, since the epsilon-greedy approach explores with a fixed probability, it may sometimes choose a suboptimal action, even if it has a lower estimated action value.\n",
        "\n",
        "- **UCB approach:** The action value estimated is based on both the sample average of the rewards obtained for each action and the uncertainty or confidence in that estimate. The UCB approach tries to balance the exploration and exploitation trade-off by choosing actions that have high estimated action values as well as high uncertainty. The optimal action is the action that has the highest estimated action value with the highest uncertainty or confidence."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
